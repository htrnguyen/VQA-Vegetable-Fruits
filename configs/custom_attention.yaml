model:
  name: "vqa_custom_attention"
  cnn:
    type: "custom"
    output_dim: 512
    pretrained: false
  lstm:
    embed_dim: 256
    hidden_dim: 512
    num_layers: 2
    dropout: 0.5
  attention:
    enabled: true
    num_heads: 8

training:
  batch_size: 64 # Lớn hơn vì model nhẹ hơn
  accumulation_steps: 1
  epochs: 100 # Nhiều epochs hơn vì train từ đầu
  optimizer:
    type: "adamw"
    lr: 5e-4 # Learning rate cao hơn
    weight_decay: 0.01
  scheduler:
    type: "cosine"
    warmup_epochs: 10
  grad_clip: 5.0
  fp16: true

data:
  max_question_length: 20
  max_answer_length: 5
  num_workers: 4
  augmentation:
    enabled: true
    color_jitter: 0.4
    random_affine: true
    random_horizontal_flip: true

logging:
  log_every: 100
  eval_every: 1000
  save_every: 5000
  keep_last: 1
