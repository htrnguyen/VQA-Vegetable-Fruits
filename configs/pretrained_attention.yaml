model:
  name: "vqa_pretrained_attention"
  cnn:
    type: "resnet50"
    output_dim: 2048
    pretrained: true
    freeze_backbone: true
  lstm:
    embed_dim: 512
    hidden_dim: 1024
    num_layers: 2
    dropout: 0.5
  attention:
    enabled: true
    num_heads: 8

training:
  batch_size: 32
  accumulation_steps: 2 # Effective batch size = 64
  epochs: 50
  optimizer:
    type: "adamw"
    lr: 1e-4
    weight_decay: 0.01
  scheduler:
    type: "cosine"
    warmup_epochs: 5
  grad_clip: 5.0
  fp16: true # Mixed precision training

data:
  max_question_length: 20
  max_answer_length: 5
  num_workers: 4
  augmentation:
    enabled: true
    color_jitter: 0.4
    random_affine: true
    random_horizontal_flip: true

logging:
  log_every: 100
  eval_every: 1000
  save_every: 5000
  keep_last: 1 # Chỉ giữ lại checkpoint tốt nhất
